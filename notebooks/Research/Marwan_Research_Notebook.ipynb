{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric Features\n",
    "The youtube datasets presents a variety of numeric variables. These variables and their relationship with the number of days a video remains on the trending page, as well as the number of days a video takes to reach the trending page.\n",
    "\n",
    "The variables given in the dataset which will be considered in this section are:\n",
    "\n",
    "- Views\n",
    "- Likes\n",
    "- Dislikes\n",
    "- Publish Date\n",
    "- Publish Time\n",
    "- Trending Date \n",
    "- Category\n",
    "\n",
    "\n",
    "## Motivation\n",
    "Our motivation regarding this section is to understand which of the variables listed above are considered by youtube's algorithm, and to identify - given these features - if it is possible to predict how soon after being published a video will reach trending, and once it reaches trending how long it will remain trending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "%matplotlib notebook\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import random\n",
    "import operator\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from statistics import *\n",
    "import concurrent.futures\n",
    "import time\n",
    "#import pyLDAvis.sklearn\n",
    "from pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\n",
    "import warnings\n",
    "import nltk\n",
    "\n",
    "\n",
    "# spaCy based imports\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# keras module for building LSTM \n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "#from keras.callbacks import EarlyStopping\n",
    "#from keras.models import Sequential\n",
    "#import keras.utils as ku \n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.utils as ku\n",
    "# set seeds for reproducability\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENG_df = pd.read_csv('../../data/data.csv')\n",
    "\n",
    "#Publish_time column contains date and time together, first I will get the correct format.\n",
    "ENG_df['publish_time'] = pd.to_datetime(ENG_df['publish_time'], errors='coerce', format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "\n",
    "#Removing any null values\n",
    "ENG_df = ENG_df[ENG_df['trending_date'].notnull()]\n",
    "ENG_df = ENG_df[ENG_df['publish_time'].notnull()]\n",
    "\n",
    "#Separating previous publish_time column into two separate columns, publish date and publish time. \n",
    "ENG_df.insert(4, 'publish_date', ENG_df['publish_time'].dt.date)\n",
    "ENG_df['publish_time'] = ENG_df['publish_time'].dt.time\n",
    "\n",
    "#splitting publish_time column into separate hour, minute and second columns\n",
    "ENG_df['publish_time'] = ENG_df['publish_time'].astype(str)\n",
    "ENG_df[['hour','minute','second']] = ENG_df.publish_time.str.split(\":\", expand=True).astype(int)\n",
    "\n",
    "#Adding like_rate, dislike_rate and comment_rate features to observe. I expect there to be a relationship between these features\n",
    "#and time to reach trending, as well as time remained on trending. \n",
    "#These features represent viewer engagement, what percentage of viewers actually like, dislike and/or comment on the videos.\n",
    "ENG_df['like_rate'] = ENG_df['likes']/ENG_df['views']*100\n",
    "ENG_df['dislike_rate'] = ENG_df['dislikes']/ENG_df['views']*100\n",
    "ENG_df['comment_rate'] = ENG_df['comment_count']/ENG_df['views']*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_init = ENG_df[ENG_df['country']== 'Country.us']\n",
    "GB_init = ENG_df[ENG_df['country']== 'Country.gb']\n",
    "CA_init = ENG_df[ENG_df['country']== 'Country.ca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addingn columns into each region's dataframe which represents the number of days a video remains trending. \n",
    "occurances_US = US_init.groupby(['video_id']).size()\n",
    "days_trending_US = occurances_US.to_frame(name = 'days_trending').reset_index()\n",
    "\n",
    "US = pd.merge(left=US_init, right=days_trending_US, left_on='video_id', right_on='video_id', how='outer')\n",
    "\n",
    "occurances_GB = GB_init.groupby(['video_id']).size()\n",
    "days_trending_GB = occurances_GB.to_frame(name = 'days_trending').reset_index()\n",
    "\n",
    "GB = pd.merge(left=GB_init, right=days_trending_GB, left_on='video_id', right_on='video_id', how='outer')\n",
    "\n",
    "occurances_CA = CA_init.groupby(['video_id']).size()\n",
    "days_trending_CA = occurances_CA.to_frame(name = 'days_trending').reset_index()\n",
    "\n",
    "CA = pd.merge(left=CA_init, right=days_trending_CA, left_on='video_id', right_on='video_id', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframes which consist of the first occurance of a video on trending, \n",
    "#as well as one for a video's last occurence on trending\n",
    "US_last = US.drop_duplicates(['video_id'], keep='last')\n",
    "US_first = US.drop_duplicates(['video_id'], keep='first')\n",
    "\n",
    "GB_last = GB.drop_duplicates(['video_id'], keep='last')\n",
    "GB_first = GB.drop_duplicates(['video_id'], keep='first')\n",
    "\n",
    "CA_last = CA.drop_duplicates(['video_id'], keep='last')\n",
    "CA_first = CA.drop_duplicates(['video_id'], keep='first')\n",
    "\n",
    "#US.set_index(['trending_date','video_id'], inplace= True)\n",
    "#US.set_index(['trending_date', 'video_id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will use sklearn's SelectKBest features on each region's dataset respectively. I will be considering, like rate, comment rate, dislike rate, hour, minute and second published. I will be assessing their applicability to days trending. Additionally, I will observe whether the number of views a video has on it's first occurance on trending is a valid predictor to the number of days a video remains on trending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6351 entries, 0 to 40948\n",
      "Data columns (total 26 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   video_id                6351 non-null   object \n",
      " 1   trending_date           6351 non-null   object \n",
      " 2   title                   6351 non-null   object \n",
      " 3   channel_title           6351 non-null   object \n",
      " 4   publish_date            6351 non-null   object \n",
      " 5   category_id             6351 non-null   int64  \n",
      " 6   publish_time            6351 non-null   object \n",
      " 7   tags                    6351 non-null   object \n",
      " 8   views                   6351 non-null   int64  \n",
      " 9   likes                   6351 non-null   int64  \n",
      " 10  dislikes                6351 non-null   int64  \n",
      " 11  comment_count           6351 non-null   int64  \n",
      " 12  thumbnail_link          6351 non-null   object \n",
      " 13  comments_disabled       6351 non-null   bool   \n",
      " 14  ratings_disabled        6351 non-null   bool   \n",
      " 15  video_error_or_removed  6351 non-null   bool   \n",
      " 16  description             6249 non-null   object \n",
      " 17  category_name           6351 non-null   object \n",
      " 18  country                 6351 non-null   object \n",
      " 19  hour                    6351 non-null   int32  \n",
      " 20  minute                  6351 non-null   int32  \n",
      " 21  second                  6351 non-null   int32  \n",
      " 22  like_rate               6351 non-null   float64\n",
      " 23  dislike_rate            6351 non-null   float64\n",
      " 24  comment_rate            6351 non-null   float64\n",
      " 25  days_trending           6351 non-null   int64  \n",
      "dtypes: bool(3), float64(3), int32(3), int64(6), object(11)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(US_first.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of        like_rate  dislike_rate  comment_rate  hour  minute  second\n",
      "6       3.755347      0.310811      0.863541    17      13       1\n",
      "13      2.475692      0.188365      0.324418     7      30       0\n",
      "20      3.523733      0.136921      0.187942    19       5      24\n",
      "27      1.831773      0.151763      0.327177    11       0       4\n",
      "33      5.441241      0.085701      0.729767    18       1      41\n",
      "...          ...           ...           ...   ...     ...     ...\n",
      "40944   4.307714      0.147344      0.579004    18      55      26\n",
      "40945   0.947428      0.039369      0.308182    15       6       8\n",
      "40946   2.176723      0.046170      0.124278     5      27      27\n",
      "40947   2.630015      0.128298      0.224197    16       3      58\n",
      "40948   2.753000      0.099225      0.257851     9       0       6\n",
      "\n",
      "[6351 rows x 6 columns]>\n",
      "<bound method NDFrame.head of 6        7\n",
      "13       7\n",
      "20       7\n",
      "27       7\n",
      "33       6\n",
      "        ..\n",
      "40944    1\n",
      "40945    1\n",
      "40946    1\n",
      "40947    1\n",
      "40948    1\n",
      "Name: days_trending, Length: 6351, dtype: int64>\n"
     ]
    }
   ],
   "source": [
    "#Beginning with the US dataset. Considering the features regarding a video on it's first occurence on trending. \n",
    "#selecting features to consider for X_first as: views, like_rate, dislike_rate, comment_count, hour of publish, minute of publish and second of publish.\n",
    "X_US_first = US_first.iloc[:,[8,22,23,24,19,20,21]]\n",
    "y_US_first = US_first.iloc[:,-1]\n",
    "\n",
    "#print(X_first.head)\n",
    "#print(Y_first.head)\n",
    "\n",
    "X_US_last = US_last.iloc[:,[22,23,24,19,20,21]]\n",
    "Y_US_last = US_last.iloc[:,-1]\n",
    "\n",
    "print(X_US_last.head)\n",
    "print(Y_US_last.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Variable     Score\n",
      "0         views  0.083957\n",
      "1     like_rate  0.047186\n",
      "2  dislike_rate  0.016911\n",
      "3  comment_rate  0.032966\n",
      "4          hour  0.019325\n",
      "5        minute  0.020259\n",
      "6        second  0.005865\n"
     ]
    }
   ],
   "source": [
    "US_first_selector = SelectKBest(score_func=mutual_info_regression, k=5)\n",
    "US_first_new = US_first_selector.fit_transform(X_US_first, y_US_first)\n",
    "#print(US_first_new[:5])\n",
    "#print(X_US_first)\n",
    "\n",
    "scores_US_first = pd.DataFrame({'Variable' : X_US_first.columns, 'Score': US_first_selector.scores_})\n",
    "#pvalue_US_first = pd.DataFrame({'Variable' : X_US_first.columns, 'p values' : US_first_selector.pvalues_})\n",
    "\n",
    "print(scores_US_first)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the best 5 features, the scores_ show that the best features to consider on the first occurance of a video on the trending page in order to predict the number of days it will remain trending are:\n",
    "1. Views\n",
    "2. Like rate\n",
    "3. Comment rate\n",
    "4. Dislike rate\n",
    "5. Hour uploaded\n",
    "\n",
    "Next I will perform the same feature selection on the last occurence of a video on the trending page. As a result of it being the last occurrence on trending, I will mitigate the views feature from selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Variable     Score\n",
      "0     like_rate  0.020920\n",
      "1  dislike_rate  0.012689\n",
      "2  comment_rate  0.011331\n",
      "3          hour  0.026092\n",
      "4        minute  0.013858\n",
      "5        second  0.000000\n"
     ]
    }
   ],
   "source": [
    "US_last_selector = SelectKBest(score_func=mutual_info_regression, k=5)\n",
    "US_last_new = US_last_selector.fit_transform(X_US_last, Y_US_last)\n",
    "\n",
    "US_scores_last = pd.DataFrame({'Variable' : X_US_last.columns, 'Score': US_last_selector.scores_})\n",
    "#print(X_last_new[:5])\n",
    "#print(X_last)\n",
    "\n",
    "print(US_scores_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the last occurrence of a video on trending, the 5 best selected features were:\n",
    "\n",
    "1. Like rate\n",
    "2. Comment rate\n",
    "3. Dislike rate\n",
    "4. Hour uploaded\n",
    "5. Minute uploaded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
